{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for visualizing attention layers during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miranda/anaconda3/envs/layout_guidance/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import abc\n",
    "import csv\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from typing import Union, Tuple, List, Callable, Dict, Optional\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "from omegaconf import OmegaConf\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, LMSDiscreteScheduler\n",
    "from my_model import unet_2d_condition\n",
    "\n",
    "from utils import compute_ca_loss, Pharse2idx, draw_box, setup_logger\n",
    "from hydra import compose, initialize\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for running inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(device, unet, vae, tokenizer, text_encoder, prompt, bboxes, phrases, cfg, logger):\n",
    "    with open('attentionmap.csv', 'a') as file:\n",
    "        column_id = ['TimeStep','AttentionUp', 'AttentionMid', 'AttentionDown']\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(column_id)\n",
    "        file.close()\n",
    "    \n",
    "    logger.info(\"Inference\")\n",
    "    logger.info(f\"Prompt: {prompt}\")\n",
    "    logger.info(f\"Phrases: {phrases}\")\n",
    "\n",
    "    # Get Object Positions\n",
    "    logger.info(\"Conver Phrases to Object Positions\")\n",
    "    object_positions = Pharse2idx(prompt, phrases)\n",
    "\n",
    "    # Encode Classifier Embeddings\n",
    "    uncond_input = tokenizer(\n",
    "        [\"\"] * cfg.inference.batch_size, padding=\"max_length\", max_length=tokenizer.model_max_length, return_tensors=\"pt\"\n",
    "    )\n",
    "    uncond_embeddings = text_encoder(uncond_input.input_ids.to(device))[0]\n",
    "\n",
    "    # Encode Prompt\n",
    "    input_ids = tokenizer(\n",
    "            [prompt] * cfg.inference.batch_size,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "    cond_embeddings = text_encoder(input_ids.input_ids.to(device))[0]\n",
    "    text_embeddings = torch.cat([uncond_embeddings, cond_embeddings])\n",
    "    generator = torch.manual_seed(cfg.inference.rand_seed)  # Seed generator to create the inital latent noise\n",
    "\n",
    "    latents = torch.randn(\n",
    "        (cfg.inference.batch_size, 4, 64, 64),\n",
    "        generator=generator,\n",
    "    ).to(device)\n",
    "\n",
    "    noise_scheduler = LMSDiscreteScheduler(beta_start=cfg.noise_schedule.beta_start, beta_end=cfg.noise_schedule.beta_end,\n",
    "                                           beta_schedule=cfg.noise_schedule.beta_schedule, num_train_timesteps=cfg.noise_schedule.num_train_timesteps)\n",
    "\n",
    "    noise_scheduler.set_timesteps(cfg.inference.timesteps)\n",
    "\n",
    "    latents = latents * noise_scheduler.init_noise_sigma\n",
    "\n",
    "    loss = torch.tensor(10000)\n",
    "\n",
    "    for index, t in enumerate(tqdm(noise_scheduler.timesteps)):\n",
    "        iteration = 0\n",
    "\n",
    "        while loss.item() / cfg.inference.loss_scale > cfg.inference.loss_threshold and iteration < cfg.inference.max_iter and index < cfg.inference.max_index_step:\n",
    "            latents = latents.requires_grad_(True)\n",
    "            latent_model_input = latents\n",
    "            latent_model_input = noise_scheduler.scale_model_input(latent_model_input, t)\n",
    "            noise_pred, attn_map_integrated_up, attn_map_integrated_mid, attn_map_integrated_down = \\\n",
    "                unet(latent_model_input, t, encoder_hidden_states=cond_embeddings)\n",
    "           \n",
    "            # update latents with guidance\n",
    "            loss = compute_ca_loss(attn_map_integrated_mid, attn_map_integrated_up, bboxes=bboxes,\n",
    "                                   object_positions=object_positions) * cfg.inference.loss_scale\n",
    "            \n",
    "            # save attentionmap\n",
    "            with open('attentionmap.csv', 'a') as file:\n",
    "                row = [index, attn_map_integrated_up.tolist(), attn_map_integrated_mid.tolist(), attn_map_integrated_down.tolist()]\n",
    "\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(row)\n",
    "                file.close()\n",
    "\n",
    "            grad_cond = torch.autograd.grad(loss.requires_grad_(True), [latents])[0]\n",
    "\n",
    "            latents = latents - grad_cond * noise_scheduler.sigmas[index] ** 2\n",
    "            iteration += 1\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            latent_model_input = torch.cat([latents] * 2)\n",
    "\n",
    "            latent_model_input = noise_scheduler.scale_model_input(latent_model_input, t)\n",
    "            noise_pred, attn_map_integrated_up, attn_map_integrated_mid, attn_map_integrated_down = \\\n",
    "                unet(latent_model_input, t, encoder_hidden_states=text_embeddings)\n",
    "            \n",
    "            with open('attentionmap.csv', 'a') as file:\n",
    "                row = [index, attn_map_integrated_up.tolist(), attn_map_integrated_mid.tolist(), attn_map_integrated_down.tolist()]\n",
    "\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(row)\n",
    "                file.close()\n",
    "            \n",
    "            noise_pred = noise_pred.sample\n",
    "\n",
    "            # perform guidance\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + cfg.inference.classifier_free_guidance * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "            latents = noise_scheduler.step(noise_pred, t, latents).prev_sample\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logger.info(\"Decode Image...\")\n",
    "        latents = 1 / 0.18215 * latents\n",
    "        image = vae.decode(latents).sample\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "        image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "        images = (image * 255).round().astype(\"uint8\")\n",
    "        pil_images = [Image.fromarray(image) for image in images]\n",
    "        return pil_images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to store and display attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_attention(cfg, attention):\n",
    "    avg_attention = [item / timestep for item in attention for timestep in range(cfg.inference.timesteps)]\n",
    "    return avg_attention\n",
    "\n",
    "def aggregate_attention(attention, res, select):\n",
    "    out = []\n",
    "    num_pixels = res ** 2\n",
    "    for item in attention:\n",
    "        if item.shape[1] == num_pixels:\n",
    "            cross_maps = item.reshape(len(examples[\"prompt\"]), -1, res, res, item.shape[-1])[select]\n",
    "\n",
    "def view_images(images, num_rows=1, offset_ratio=0.02):\n",
    "    if type(images) is list:\n",
    "        num_empty = len(images) % num_rows\n",
    "    elif images.ndim == 4:\n",
    "        num_empty = images.shape[0] % num_rows\n",
    "    else:\n",
    "        images = [images]\n",
    "        num_empty = 0\n",
    "\n",
    "    empty_images = np.ones(images[0].shape, dtype=np.uint8) * 255\n",
    "    images = [image.astype(np.uint8) for image in images] + [empty_images] * num_empty\n",
    "    num_items = len(images)\n",
    "\n",
    "    h, w, c = images[0].shape\n",
    "    offset = int(h * offset_ratio)\n",
    "    num_cols = num_items // num_rows\n",
    "    image_ = np.ones((h * num_rows + offset * (num_rows - 1),\n",
    "                      w * num_cols + offset * (num_cols - 1), 3), dtype=np.uint8) * 255\n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_cols):\n",
    "            image_[i * (h + offset): i * (h + offset) + h:, j * (w + offset): j * (w + offset) + w] = images[\n",
    "                i * num_cols + j]\n",
    "\n",
    "    pil_img = Image.fromarray(image_)\n",
    "    display(pil_img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3922/3351101075.py:2: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  initialize(config_path=\"conf\")\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 1.95 GiB total capacity; 1.49 GiB already allocated; 12.38 MiB free; 1.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m vae \u001b[39m=\u001b[39m AutoencoderKL\u001b[39m.\u001b[39mfrom_pretrained(cfg\u001b[39m.\u001b[39mgeneral\u001b[39m.\u001b[39mmodel_path, subfolder\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvae\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m unet\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     16\u001b[0m text_encoder\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m vae\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/layout_guidance/lib/python3.8/site-packages/torch/nn/modules/module.py:989\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    986\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    987\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 989\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/anaconda3/envs/layout_guidance/lib/python3.8/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    643\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/layout_guidance/lib/python3.8/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    643\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 641 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/layout_guidance/lib/python3.8/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    643\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/layout_guidance/lib/python3.8/site-packages/torch/nn/modules/module.py:664\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 664\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    665\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    666\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/layout_guidance/lib/python3.8/site-packages/torch/nn/modules/module.py:987\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    985\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    986\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 987\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 1.95 GiB total capacity; 1.49 GiB already allocated; 12.38 MiB free; 1.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Compose configuration file\n",
    "initialize(config_path=\"conf\")\n",
    "cfg = compose(config_name=\"base_config\")\n",
    "\n",
    "# Build model\n",
    "with open(cfg.general.unet_config) as f:\n",
    "    unet_config = json.load(f)\n",
    "unet = unet_2d_condition.UNet2DConditionModel(**unet_config).from_pretrained(cfg.general.model_path, subfolder=\"unet\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(cfg.general.model_path, subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(cfg.general.model_path, subfolder=\"text_encoder\")\n",
    "vae = AutoencoderKL.from_pretrained(cfg.general.model_path, subfolder=\"vae\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "unet.to(device)\n",
    "text_encoder.to(device)\n",
    "vae.to(device)\n",
    "\n",
    "# Prepare the save path\n",
    "if not os.path.exists(cfg.general.save_path):\n",
    "    os.makedirs(cfg.general.save_path)\n",
    "logger = setup_logger(cfg.general.save_path, __name__)\n",
    "\n",
    "logger.info(cfg)\n",
    "# Save cfg\n",
    "logger.info(\"save config to {}\".format(os.path.join(cfg.general.save_path, 'config.yaml')))\n",
    "OmegaConf.save(cfg, os.path.join(cfg.general.save_path, 'config.yaml'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "examples = {\"prompt\": \"A hello kitty toy is playing with a purple ball.\",\n",
    "            \"phrases\": \"hello kitty; ball\",\n",
    "            \"bboxes\": [[[0.1, 0.2, 0.5, 0.8]], [[0.75, 0.6, 0.95, 0.8]]],\n",
    "            'save_path': cfg.general.save_path\n",
    "            }\n",
    "\n",
    "# Inference\n",
    "pil_images = inference(device, unet, vae, tokenizer, text_encoder, examples['prompt'], examples['bboxes'], examples['phrases'], cfg, logger)\n",
    "\n",
    "view_images(np.stack(pil_images, axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save example images\n",
    "for index, pil_image in enumerate(pil_images):\n",
    "    image_path = os.path.join(cfg.general.save_path, 'example_{}.png'.format(index))\n",
    "    logger.info('save example image to {}'.format(image_path))\n",
    "    draw_box(pil_image, examples['bboxes'], examples['phrases'], image_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
