{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Map Editing\n",
    "\n",
    "If you want to mask of focus on specific elements of the attention map you have to go to models/transformer.py and edit either the mask (line 86) or the focus (line 87) variable. The options are: None, uniform, pc_to_pc, pc_to_pc_diag, img_to_pc, pc_to_img, img_to_img, cross_attention. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import imageio\n",
    "import importlib\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torch\n",
    "import nopdb\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from point_e.diffusion.configs import DIFFUSION_CONFIGS, diffusion_from_config\n",
    "from point_e.diffusion.sampler import PointCloudSampler\n",
    "from point_e.models.download import load_checkpoint\n",
    "from point_e.models.configs import MODEL_CONFIGS, model_from_config\n",
    "from point_e.util.plotting import plot_point_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lcur1728/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/lcur1728/.local/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda20CUDACachingAllocator9allocatorE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating upsample model...\n",
      "downloading base checkpoint...\n",
      "downloading upsampler checkpoint...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('creating base model...')\n",
    "base_name = 'base40M' # use base300M or base1B for better results\n",
    "base_model = model_from_config(MODEL_CONFIGS[base_name], device)\n",
    "base_model.eval()\n",
    "base_diffusion = diffusion_from_config(DIFFUSION_CONFIGS[base_name])\n",
    "\n",
    "print('creating upsample model...')\n",
    "upsampler_model = model_from_config(MODEL_CONFIGS['upsample'], device)\n",
    "upsampler_model.eval()\n",
    "upsampler_diffusion = diffusion_from_config(DIFFUSION_CONFIGS['upsample'])\n",
    "\n",
    "print('downloading base checkpoint...')\n",
    "base_model.load_state_dict(load_checkpoint(base_name, device))\n",
    "\n",
    "print('downloading upsampler checkpoint...')\n",
    "upsampler_model.load_state_dict(load_checkpoint('upsample', device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = PointCloudSampler(\n",
    "    device=device,\n",
    "    models=[base_model, upsampler_model],\n",
    "    diffusions=[base_diffusion, upsampler_diffusion],\n",
    "    num_points=[1024, 4096 - 1024],\n",
    "    aux_channels=['R', 'G', 'B'],\n",
    "    guidance_scale=[3.0, 3.0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image to condition on.\n",
    "img = Image.open('example_data/cube_stack.jpg')\n",
    "import sys\n",
    "\n",
    "def sample_from_model(breakpoint):\n",
    "    samples = None\n",
    "    k = 0\n",
    "    with nopdb.capture_call(base_model.backbone.resblocks[-1].attn.attention.forward) as attn_call:\n",
    "        for x in tqdm(sampler.sample_batch_progressive(batch_size=1, model_kwargs=dict(images=[img]))):\n",
    "            if x.shape[2] == 1024:\n",
    "                samples = x\n",
    "                if k == breakpoint:\n",
    "                    break\n",
    "                k += 1\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "    attention_probs = attn_call.locals['weight'][0]\n",
    "\n",
    "    # Average across all heads\n",
    "    avg_attn = torch.mean(attention_probs, dim = 0)\n",
    "    \n",
    "    # Est. self attention\n",
    "    pc_self_attn = avg_attn[257:, 257:]\n",
    "    \n",
    "    # Est. cross attention\n",
    "    img_self_attn = avg_attn[:257, :257]\n",
    "\n",
    "    # Est. cross attention\n",
    "    img_to_pc_cross_attn = avg_attn[:257, 257:]\n",
    "\n",
    "    # Est. cross attention\n",
    "    pc_to_img_cross_attn = avg_attn[257:, :257]\n",
    "\n",
    "    pc_self_attn = pc_self_attn.cpu()\n",
    "    img_self_attn = img_self_attn.cpu()\n",
    "    img_to_pc_cross_attn = img_to_pc_cross_attn.cpu()\n",
    "    pc_to_img_cross_attn = pc_to_img_cross_attn.cpu()\n",
    "    avg_attn = avg_attn.cpu()\n",
    "    \n",
    "    return pc_self_attn, img_self_attn, img_to_pc_cross_attn, pc_to_img_cross_attn, avg_attn, samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "772e02d937674c889d2d2461c5886f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "breakpoints = [0, 10, 20, 30, 40, 50, 60, -1]\n",
    "time = [0, 1, 2, 3, 4, 5, 6, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1c37aaa72044618a00cbe5e5b74817e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91330c725d1d448e89666e20b8498199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "388ece7426ce44049f0a3b19054e1be0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ad11e5ca192432cb561f0ce59ee7822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a6b67057ae428abeb658d8634c0739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7666f4d5ae684a58b744f73bd00d05fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e8dbc3b9659458eaa9ee7eb6b390897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c33e5804a7294ec2bd6c9bfb54f0673e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Run model \n",
    "\"\"\"\n",
    "\n",
    "frames = []\n",
    "\n",
    "for k in breakpoints:\n",
    "    pc_self_attn, img_self_attn, img_to_pc_cross_attn, pc_to_img_cross_attn, avg_attn, samples = sample_from_model(k)\n",
    "    \n",
    "    ax = sns.heatmap(pc_self_attn, cmap = 'rocket_r', cbar=False)\n",
    "    plt.title('itterations = ' + str(k))\n",
    "    plt.savefig('Figures/pc2pc/' +str(k) + '.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    ax = sns.heatmap(img_self_attn, cmap = 'rocket_r', cbar=False)\n",
    "    plt.title('itterations = ' + str(k))\n",
    "    plt.savefig('Figures/img2img/' +str(k) + '.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    ax = sns.heatmap(img_to_pc_cross_attn, cmap = 'rocket_r', cbar=False)\n",
    "    plt.title('itterations = ' + str(k))\n",
    "    plt.savefig('Figures/img2pc/' +str(k) + '.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    ax = sns.heatmap(pc_to_img_cross_attn, cmap = 'rocket_r', cbar=False)\n",
    "    plt.title('itterations = ' + str(k))\n",
    "    plt.savefig('Figures/pc2img/' +str(k) + '.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    ax = sns.heatmap(avg_attn, cmap = 'rocket_r', cbar=False)\n",
    "    plt.title('itterations = ' + str(k))\n",
    "    plt.savefig('Figures/full/' +str(k) + '.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    pc = sampler.output_to_point_clouds(samples)[0]\n",
    "    ax = plot_point_cloud(pc, grid_size=3, fixed_bounds=((-0.75, -0.75, -0.75),(0.75, 0.75, 0.75)))\n",
    "    plt.title('iterations = ' + str(k))\n",
    "    plt.savefig('Figures/Diffusion_gif_2/fig' + str(k) + '.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-b04075ddebef>:8: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imageio.imread(images[t])\n",
      "<ipython-input-8-b04075ddebef>:18: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imageio.imread(images[t])\n"
     ]
    }
   ],
   "source": [
    "# \"\"\"\n",
    "# Make Gifs\n",
    "# \"\"\"\n",
    "\n",
    "images = ['Figures/Diffusion_gif_2/fig' + str(k) + '.png' for k in breakpoints]\n",
    "frames = []\n",
    "for t in time:\n",
    "    image = imageio.imread(images[t])\n",
    "    frames.append(image)\n",
    "\n",
    "imageio.mimsave('Figures/Diffusion_gif_2/Diffusion_at_scale.gif', # output gif\n",
    "                frames,          # array of input frames\n",
    "                duration = 2000)         # optional: frames per second  \n",
    "\n",
    "images = ['Figures/full/' + str(k) + '.png' for k in breakpoints]\n",
    "frames = []\n",
    "for t in time:\n",
    "    image = imageio.imread(images[t])\n",
    "    frames.append(image)\n",
    "    \n",
    "imageio.mimsave('Figures/full/full_attention.gif', frames, duration=2000)\n",
    "\n",
    "images = ['Figures/pc2pc/' + str(k) + '.png' for k in breakpoints]\n",
    "frames = []\n",
    "for t in time:\n",
    "    image = imageio.v2.imread(images[t])\n",
    "    frames.append(image)\n",
    "    \n",
    "imageio.mimsave('Figures/pc2pc/pc2pc_self_attention.gif', frames, duration=2000)\n",
    "\n",
    "images = ['Figures/img2img/' + str(k) + '.png' for k in breakpoints]\n",
    "frames = []\n",
    "for t in time:\n",
    "    image = imageio.v2.imread(images[t])\n",
    "    frames.append(image)\n",
    "    \n",
    "imageio.mimsave('Figures/img2img/img2img_self_attention.gif', frames, duration=2000)\n",
    "\n",
    "images = ['Figures/img2pc/' + str(k) + '.png' for k in breakpoints]\n",
    "frames = []\n",
    "for t in time:\n",
    "    image = imageio.v2.imread(images[t])\n",
    "    frames.append(image)\n",
    "    \n",
    "imageio.mimsave('Figures/img2pc/img2pc_attention.gif', frames, duration=2000)\n",
    "\n",
    "images = ['Figures/pc2img/' + str(k) + '.png' for k in breakpoints]\n",
    "frames = []\n",
    "for t in time:\n",
    "    image = imageio.v2.imread(images[t])\n",
    "    frames.append(image)\n",
    "    \n",
    "imageio.mimsave('Figures/pc2img/pc2img_attention.gif', frames, duration=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b270b0f43bc427bcab7703c037711644cc480aac7c1cc8d2940cfaf0b447ee2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
