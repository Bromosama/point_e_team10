{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Map Editing\n",
    "\n",
    "If you want to mask of focus on specific elements of the attention map you have to go to models/transformer.py and edit either the mask (line 86) or the focus (line 87) variable. The options are: None, uniform, pc_to_pc, pc_to_pc_diag, img_to_pc, pc_to_img, img_to_img, cross_attention. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import seaborn as sns\n",
    "import imageio\n",
    "import importlib\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torch\n",
    "import nopdb\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Detect local paths\n",
    "local_path = !pwd\n",
    "local_path = local_path[0][:-5]\n",
    "sys.path.append(local_path + 'src/')\n",
    "\n",
    "from point_e.diffusion.configs import DIFFUSION_CONFIGS, diffusion_from_config\n",
    "from point_e.diffusion.sampler import PointCloudSampler\n",
    "from point_e.models.download import load_checkpoint\n",
    "from point_e.models.configs import MODEL_CONFIGS, model_from_config\n",
    "from point_e.util.plotting import plot_point_cloud\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Path\n",
    "path = '../src/point_e/examples'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-settings:\n",
    "Choose either an option for mask or focus\n",
    "|        |  |\n",
    "|----------------|:----------------------------------------------------------------------------------------------------------------------:|\n",
    "| `mask`:        |  Set the mask here, options: None, uniform, pc_to_pc, pc_to_pc_diag, img_to_pc, pc_to_img, img_to_img, cross_attention |\n",
    "| `focus`:       | Set the focus here, options: None, uniform, pc_to_pc, pc_to_pc_diag, img_to_pc, pc_to_img, img_to_img, cross_attention |\n",
    "| `base_name`:   | Set name of base model here, use base300M or base1B for better results                                                 |\n",
    "| `img`:         | Load the image to condition on                                                                                         |\n",
    "| `breakpoints`: | Define the breakpoints for attention sampling                                                                          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = \"uniform\"\n",
    "focus = \"None\"\n",
    "base_name = 'base40M' # use base300M or base1B for better results\n",
    "img = Image.open(path +'/example_data/cube_stack.jpg')\n",
    "breakpoints = [0, 10, 20, 30, 40, 50, 60, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('creating base model...')\n",
    "\n",
    "base_config = MODEL_CONFIGS[base_name].copy()\n",
    "base_config['mask'] = mask\n",
    "base_config['focus'] = focus\n",
    "base_model = model_from_config(base_config, device)\n",
    "base_model.eval()\n",
    "base_diffusion = diffusion_from_config(DIFFUSION_CONFIGS[base_name])\n",
    "\n",
    "print('creating upsample model...')\n",
    "upsampler_model = model_from_config(MODEL_CONFIGS['upsample'], device)\n",
    "upsampler_model.eval()\n",
    "upsampler_diffusion = diffusion_from_config(DIFFUSION_CONFIGS['upsample'])\n",
    "\n",
    "print('downloading base checkpoint...')\n",
    "base_model.load_state_dict(load_checkpoint(base_name, device))\n",
    "\n",
    "print('downloading upsampler checkpoint...')\n",
    "upsampler_model.load_state_dict(load_checkpoint('upsample', device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = PointCloudSampler(\n",
    "    device=device,\n",
    "    models=[base_model, upsampler_model],\n",
    "    diffusions=[base_diffusion, upsampler_diffusion],\n",
    "    num_points=[1024, 4096 - 1024],\n",
    "    aux_channels=['R', 'G', 'B'],\n",
    "    guidance_scale=[3.0, 3.0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_model(breakpoint):\n",
    "    samples = None\n",
    "    k = 0\n",
    "    with nopdb.capture_call(base_model.backbone.resblocks[-1].attn.attention.forward) as attn_call:\n",
    "        for x in tqdm(sampler.sample_batch_progressive(batch_size=1, model_kwargs=dict(images=[img]))):\n",
    "            if x.shape[2] == 1024:\n",
    "                samples = x\n",
    "                if k == breakpoint:\n",
    "                    break\n",
    "                k += 1\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "    attention_probs = attn_call.locals['weight'][0]\n",
    "\n",
    "    # Average across all heads\n",
    "    avg_attn = torch.mean(attention_probs, dim = 0)\n",
    "    \n",
    "    # Est. self attention\n",
    "    pc_self_attn = avg_attn[257:, 257:]\n",
    "    \n",
    "    # Est. cross attention\n",
    "    img_self_attn = avg_attn[:257, :257]\n",
    "\n",
    "    # Est. cross attention\n",
    "    img_to_pc_cross_attn = avg_attn[:257, 257:]\n",
    "\n",
    "    # Est. cross attention\n",
    "    pc_to_img_cross_attn = avg_attn[257:, :257]\n",
    "\n",
    "    pc_self_attn = pc_self_attn.cpu()\n",
    "    img_self_attn = img_self_attn.cpu()\n",
    "    img_to_pc_cross_attn = img_to_pc_cross_attn.cpu()\n",
    "    pc_to_img_cross_attn = pc_to_img_cross_attn.cpu()\n",
    "    avg_attn = avg_attn.cpu()\n",
    "    \n",
    "    return pc_self_attn, img_self_attn, img_to_pc_cross_attn, pc_to_img_cross_attn, avg_attn, samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Model and save visualizations of the different attention maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to save figures to\n",
    "if mask != \"None\":\n",
    "    fig_path = path +'/Figures/Attention_Edit/Mask_' + mask\n",
    "else:\n",
    "    fig_path = path +'/Figures/Attention_Edit/Focus_' + focus\n",
    "\n",
    "# Initialize directories if not exists\n",
    "vis = ['pc2pc', 'img2img', 'img2pc', 'pc2img', 'full', 'pointcloud']\n",
    "for i in vis:\n",
    "    os.makedirs(fig_path + '/' + i + '/frames', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in breakpoints:\n",
    "    samples = sample_from_model(k)\n",
    "    for idx, i in enumerate(vis):\n",
    "        if i == 'pointcloud':\n",
    "            pc = sampler.output_to_point_clouds(samples[idx])[0]\n",
    "            ax = plot_point_cloud(pc, grid_size=3, fixed_bounds=((-0.75, -0.75, -0.75),(0.75, 0.75, 0.75)))\n",
    "            plt.title('iterations = ' + str(k))\n",
    "            plt.savefig(fig_path + '/pointcloud/frames/' + str(k) + '.png', bbox_inches='tight')\n",
    "            plt.close()\n",
    "        else:\n",
    "            ax = sns.heatmap(samples[idx], cmap = 'rocket_r', cbar=False)\n",
    "            plt.title('iterations = ' + str(k))\n",
    "            plt.savefig(fig_path + '/' + i + '/frames/' + str(k) + '.png', bbox_inches='tight')\n",
    "            plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create GIFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = range(len(breakpoints))\n",
    "\n",
    "for idx, i in enumerate(vis):\n",
    "    if idx > 3:\n",
    "        images = [fig_path + '/' + i + '/frames/' + str(k) + '.png' for k in breakpoints]\n",
    "        frames = [imageio.imread(image) for image in images]\n",
    "    else:\n",
    "        images = [fig_path + '/' + i + '/frames/' + str(k) + '.png' for k in breakpoints]\n",
    "        frames = [imageio.v2.imread(image) for image in images]\n",
    "        \n",
    "    imageio.mimsave(fig_path + '/' + i + '/' + i + '_attention.gif', frames, duration=2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b270b0f43bc427bcab7703c037711644cc480aac7c1cc8d2940cfaf0b447ee2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
