{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import nopdb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from point_e.diffusion.configs import DIFFUSION_CONFIGS, diffusion_from_config\n",
    "from point_e.diffusion.sampler import PointCloudSampler\n",
    "from point_e.models.download import load_checkpoint\n",
    "from point_e.models.configs import MODEL_CONFIGS, model_from_config\n",
    "from point_e.util.plotting import plot_point_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating base model...\n",
      "creating upsample model...\n",
      "downloading base checkpoint...\n",
      "downloading upsampler checkpoint...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('creating base model...')\n",
    "base_name = 'base40M' # use base300M or base1B for better results\n",
    "base_model = model_from_config(MODEL_CONFIGS[base_name], device)\n",
    "base_model.eval()\n",
    "base_diffusion = diffusion_from_config(DIFFUSION_CONFIGS[base_name])\n",
    "\n",
    "print('creating upsample model...')\n",
    "upsampler_model = model_from_config(MODEL_CONFIGS['upsample'], device)\n",
    "upsampler_model.eval()\n",
    "upsampler_diffusion = diffusion_from_config(DIFFUSION_CONFIGS['upsample'])\n",
    "\n",
    "print('downloading base checkpoint...')\n",
    "base_model.load_state_dict(load_checkpoint(base_name, device))\n",
    "\n",
    "print('downloading upsampler checkpoint...')\n",
    "upsampler_model.load_state_dict(load_checkpoint('upsample', device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = PointCloudSampler(\n",
    "    device=device,\n",
    "    models=[base_model, upsampler_model],\n",
    "    diffusions=[base_diffusion, upsampler_diffusion],\n",
    "    num_points=[1024, 4096 - 1024],\n",
    "    aux_channels=['R', 'G', 'B'],\n",
    "    guidance_scale=[3.0, 3.0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb0fee503012421bacb1d65223c5dc0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load an image to condition on.\n",
    "img = Image.open('example_data/cube_stack.jpg')\n",
    "\n",
    "# Produce a sample from the model.\n",
    "samples = None\n",
    "with nopdb.capture_call(base_model.backbone.resblocks[-1].attn.attention.forward) as attn_call:\n",
    "    for x in tqdm(sampler.sample_batch_progressive(batch_size=1, model_kwargs=dict(images=[img]))):\n",
    "        samples = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['self', 'qkv', 'bs', 'n_ctx', 'width', 'attn_ch', 'scale', 'q', 'k', 'v', 'weight', 'wdtype'])\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "print(attn_call.locals.keys())\n",
    "print(attn_call.locals['attn_ch'])\n",
    "# print(attn_call.locals['x'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1281, 8, 64])\n",
      "torch.Size([2, 1281, 8, 64])\n",
      "torch.Size([2, 1281, 8, 64])\n",
      "torch.Size([2, 8, 1281, 1281])\n"
     ]
    }
   ],
   "source": [
    "print(attn_call.locals['q'].shape)\n",
    "print(attn_call.locals['k'].shape)\n",
    "print(attn_call.locals['v'].shape)\n",
    "print(attn_call.locals['weight'].shape)\n",
    "batch, heads, target, source = attn_call.locals['weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1281, 1281])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import math\n",
    "scale = 1 / math.sqrt(math.sqrt(attn_call.locals['attn_ch']))\n",
    "\n",
    "def reshape_heads_to_batch_dim(tensor):\n",
    "        batch_size, seq_len, heads, dim = tensor.shape\n",
    "        tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * heads, seq_len, dim)\n",
    "        return tensor\n",
    "#Manually extract the query and key tensors and combine them as in transformer module, to obtain the attention map.\n",
    "new_q = reshape_heads_to_batch_dim(attn_call.locals['q'])\n",
    "new_k = reshape_heads_to_batch_dim(attn_call.locals['k'])\n",
    "attention_scores = torch.einsum(\"b i d, b j d -> b i j\", new_q, new_k) * scale\n",
    "\n",
    "attention_probs = attention_scores.softmax(dim=-1)\n",
    "print(attention_probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.9516e-02, 6.8748e-05, 9.5237e-04,  ..., 1.1808e-04,\n",
      "          1.1010e-06, 1.8025e-05],\n",
      "         [3.0395e-04, 8.0177e-03, 1.9105e-04,  ..., 1.7302e-06,\n",
      "          3.3840e-06, 1.1612e-06],\n",
      "         [9.2774e-04, 1.0865e-04, 8.8315e-04,  ..., 9.0119e-05,\n",
      "          6.5337e-06, 8.9934e-06],\n",
      "         ...,\n",
      "         [6.3521e-06, 4.5698e-08, 4.6456e-08,  ..., 3.5422e-02,\n",
      "          4.3947e-05, 2.9925e-05],\n",
      "         [1.6877e-09, 4.0746e-09, 3.5010e-10,  ..., 2.7414e-06,\n",
      "          4.8975e-02, 1.6963e-05],\n",
      "         [2.6988e-07, 6.0006e-08, 1.0788e-08,  ..., 1.5673e-05,\n",
      "          2.8754e-04, 1.1290e-02]],\n",
      "\n",
      "        [[9.8665e-02, 2.7145e-04, 2.1516e-03,  ..., 1.3662e-05,\n",
      "          1.1435e-06, 9.4908e-06],\n",
      "         [3.6014e-04, 6.7240e-03, 2.2407e-03,  ..., 2.2885e-05,\n",
      "          1.0653e-04, 2.2251e-05],\n",
      "         [2.3039e-04, 2.9884e-04, 2.9164e-03,  ..., 5.8499e-07,\n",
      "          9.3270e-06, 4.3015e-06],\n",
      "         ...,\n",
      "         [6.4138e-09, 1.8667e-09, 8.8785e-10,  ..., 1.1489e-01,\n",
      "          1.2393e-07, 6.4442e-10],\n",
      "         [1.6586e-10, 6.2976e-10, 4.2329e-10,  ..., 5.1156e-09,\n",
      "          9.4387e-02, 2.5344e-08],\n",
      "         [3.1783e-07, 1.5736e-07, 1.6760e-07,  ..., 2.5921e-09,\n",
      "          8.1696e-07, 1.3754e-02]],\n",
      "\n",
      "        [[7.1030e-06, 1.0631e-07, 2.8450e-07,  ..., 5.6289e-06,\n",
      "          6.6271e-07, 2.7289e-05],\n",
      "         [1.1144e-08, 3.0436e-07, 1.7594e-09,  ..., 4.3651e-06,\n",
      "          6.9013e-06, 2.6350e-07],\n",
      "         [5.2127e-11, 9.8120e-13, 1.0214e-12,  ..., 1.2733e-05,\n",
      "          1.0452e-07, 7.8368e-09],\n",
      "         ...,\n",
      "         [9.4169e-16, 9.8210e-16, 2.5501e-15,  ..., 3.4737e-01,\n",
      "          7.8806e-18, 1.0405e-24],\n",
      "         [3.9676e-17, 1.6104e-15, 5.4747e-16,  ..., 2.8780e-18,\n",
      "          5.4763e-01, 2.6847e-15],\n",
      "         [3.7475e-16, 9.3509e-18, 1.0234e-17,  ..., 7.4729e-26,\n",
      "          6.2875e-16, 2.3727e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[2.4346e-09, 1.5094e-08, 1.5094e-08,  ..., 1.4913e-05,\n",
      "          1.2192e-05, 1.3123e-05],\n",
      "         [5.6542e-10, 1.4573e-06, 1.4573e-06,  ..., 3.8887e-05,\n",
      "          2.4739e-06, 1.0177e-06],\n",
      "         [5.6542e-10, 1.4573e-06, 1.4573e-06,  ..., 3.8887e-05,\n",
      "          2.4739e-06, 1.0177e-06],\n",
      "         ...,\n",
      "         [2.8474e-09, 2.5079e-12, 2.5079e-12,  ..., 6.9064e-02,\n",
      "          3.3774e-12, 6.5591e-14],\n",
      "         [1.1563e-10, 3.2016e-11, 3.2016e-11,  ..., 3.1380e-13,\n",
      "          2.0467e-01, 2.2499e-09],\n",
      "         [5.7216e-09, 5.1277e-11, 5.1277e-11,  ..., 9.5317e-14,\n",
      "          4.9815e-09, 4.1726e-02]],\n",
      "\n",
      "        [[9.9368e-01, 1.7783e-07, 1.7783e-07,  ..., 1.8123e-07,\n",
      "          1.1627e-08, 1.5930e-08],\n",
      "         [8.0954e-04, 3.9023e-03, 3.9023e-03,  ..., 2.2342e-08,\n",
      "          2.1984e-07, 4.1734e-08],\n",
      "         [8.0954e-04, 3.9023e-03, 3.9023e-03,  ..., 2.2342e-08,\n",
      "          2.1984e-07, 4.1734e-08],\n",
      "         ...,\n",
      "         [3.5485e-08, 7.8786e-09, 7.8786e-09,  ..., 2.7642e-02,\n",
      "          3.2526e-06, 1.0595e-07],\n",
      "         [6.1100e-08, 5.4631e-09, 5.4631e-09,  ..., 1.8780e-06,\n",
      "          9.4740e-02, 8.8618e-06],\n",
      "         [8.9364e-08, 1.4993e-08, 1.4993e-08,  ..., 4.4990e-07,\n",
      "          2.8588e-05, 3.5142e-02]],\n",
      "\n",
      "        [[6.5277e-07, 4.1809e-05, 4.1809e-05,  ..., 5.9331e-05,\n",
      "          1.9731e-05, 2.3331e-03],\n",
      "         [1.1474e-09, 1.1743e-05, 1.1743e-05,  ..., 1.0717e-03,\n",
      "          3.2500e-04, 1.2525e-04],\n",
      "         [1.1474e-09, 1.1743e-05, 1.1743e-05,  ..., 1.0717e-03,\n",
      "          3.2500e-04, 1.2525e-04],\n",
      "         ...,\n",
      "         [1.8611e-11, 8.0955e-13, 8.0955e-13,  ..., 2.7699e-01,\n",
      "          7.4309e-14, 3.0883e-18],\n",
      "         [4.0855e-13, 1.2295e-12, 1.2295e-12,  ..., 7.5384e-14,\n",
      "          4.9881e-01, 1.7836e-11],\n",
      "         [3.7374e-10, 1.1857e-10, 1.1857e-10,  ..., 3.3210e-17,\n",
      "          8.7587e-11, 1.5251e-01]]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(attention_probs)\n",
    "def compute_ca_loss(attn_map, bboxes, object_positions):\n",
    "    loss = 0\n",
    "    object_number = len(bboxes)\n",
    "    if object_number == 0:\n",
    "        return torch.tensor(0).float().cuda() if torch.cuda.is_available() else torch.tensor(0).float()\n",
    "    for attn_map_integrated in attn_map:\n",
    "        #This chunks the attention mid from [16,1281,1281] to [8,1281,1281]. We pick the second half. Why?\n",
    "        attn_map = attn_map_integrated.chunk(2)[1]\n",
    "        #Then we extract the dimensions. b= 8 i=1281 j=1281\n",
    "        b, i, j = attn_map.shape\n",
    "        '''\n",
    "        For creating the dimensions of the mask we need to think what makes sense in 3D space...\n",
    "        For 2D case we can just sqrt the latent dimension to create a square, because images are squares.\n",
    "        Then represent the mask as that square, but now we have 3D space...\n",
    "        Furthermore, we have that 257 dimensions are added because 256 from the image embedding and 1 from the timestep.\n",
    "        So the embedding dimensions are not only containing information about the point cloud. \n",
    "        '''\n",
    "        H = W = D = int(math.sqrt(i))\n",
    "        #Loop for the amount of objects (basically for how many bounding boxes we created)\n",
    "        for obj_idx in range(object_number):\n",
    "            obj_loss = 0 #per object \"loss\"\n",
    "            #We create a mask of all zeros using the sqrt i dimension, in this case \n",
    "            mask = torch.zeros(size=(H, W)).cuda() if torch.cuda.is_available() else torch.zeros(size=(H, W))\n",
    "            for obj_box in bboxes[obj_idx]:\n",
    "                #Extract the corners of the bounding boxes and set the mask matrix to 1 at locations inside the bounding box.\n",
    "                x_min, y_min, x_max, y_max = int(obj_box[0] * W), \\\n",
    "                    int(obj_box[1] * H), int(obj_box[2] * W), int(obj_box[3] * H)\n",
    "                mask[y_min: y_max, x_min: x_max] = 1\n",
    "            #Object_position in example case looks like: [[2,3],[10]]\n",
    "            for obj_position in object_positions[obj_idx]: #(1) 2 3 (2) 10\n",
    "                #Third dimension corresponds then to attention map of words. Picking the location of the word that has a bounding box means getting that specific attention for that word.\n",
    "                #Originally attn_map[:,:,obj_position] has shape [4,64,1]\n",
    "                ca_map_obj = attn_map[:, :, obj_position].reshape(b, H, W) #Reshape the specific attention map of that word into [4,8,8]\n",
    "                '''\n",
    "                obj_position is an integer which corresponds to the index in the query of that specific word. So here we assume that selecting the index will give us the attention\n",
    "                map of each word in the query. But why is it length 77?\n",
    "                '''\n",
    "                #Multiply this specific attention map with the mask, merge the final two dimensions and sum along them. We divide by the specific attention map of this particular word.\n",
    "                activation_value = (ca_map_obj * mask).reshape(b, -1).sum(dim=-1)/ca_map_obj.reshape(b, -1).sum(dim=-1)\n",
    "                #Intuitively: We amplify attention values of this specific object within the bounding box (indicated by mask), to then guide to model to attend more to this area for this object?\n",
    "                obj_loss += torch.mean((1 - activation_value) ** 2) #if for example an object has multiple words associated with it like \"hello kitty\", we sum the \"losses\" of both words.\n",
    "            loss += (obj_loss/len(object_positions[obj_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sampler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4038ef9cff2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_to_point_clouds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_point_cloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_bounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sampler' is not defined"
     ]
    }
   ],
   "source": [
    "pc = sampler.output_to_point_clouds(samples)[0]\n",
    "fig = plot_point_cloud(pc, grid_size=3, fixed_bounds=((-0.75, -0.75, -0.75),(0.75, 0.75, 0.75)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('3.9.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b270b0f43bc427bcab7703c037711644cc480aac7c1cc8d2940cfaf0b447ee2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
